# Municipal Chronicle of Star√° Ves (1923-1991)

A digitized 400-page Czech municipal chronicle covering 150 years of local history, converted to a searchable website.

## üåê Live Demo
[https://adbo28.github.io/kronika/](https://adbo28.github.io/kronika/)

## üìñ About
Converts a historical Czech municipal chronicle (1923-1991) from PDF to a responsive website with hierarchical navigation, full-text search, and mobile-friendly design.

## üõ†Ô∏è Tech Stack
- **PDF Processing**: PyMuPDF4LLM
- **Site Generator**: MkDocs with Material theme
- **Package Manager**: uv
- **Deployment**: GitHub Pages

## üöÄ Quick Start
```bash
git clone https://github.com/adbo28/kronika.git
cd kronika
uv sync
cd docs-site
uv run mkdocs serve
```

### Deploy to GitHub Pages 
```
cd docs-site
uv run mkdocs gh-deploy
```

## Data Files
The processing pipeline consumes and generates several data files located in the `data/` directory.

### Source Data
-   `kronika.md`: The initial raw markdown file converted from the source PDF.
-   `kronika_plus.md`: An enriched version of `kronika.md` with additional metadata and anchor links. This file is generated by `md_process.py`.
-   `chapter_mapping.jsonl`: A manually created file that defines the chapter structure of the chronicle.
-   `event_base_form.jsonl`, `name_base_form.jsonl`: Contain mappings from original names and events to their base forms. These are used to normalize data for indexing.
-   `first_names.txt`, `names_with_dots.txt`: Lists of names used to improve name recognition accuracy.
-   `pattern_rules.csv`: A set of rules to fine-tune entity recognition.
-   `bw_list.jsonl`: A blacklist/whitelist for entity recognition.

### Generated Data
-   `index_data.jsonl`: A JSONL file containing all recognized entities (names, years, events, addresses) and their context. Generated by `md_process.py`.
-   `global_index.json`: A comprehensive JSON file that aggregates all entities and their occurrences across all chapters. Generated by `md_split.py`.

## Scripts

### Main Scripts
-   `md_create.py`: Converts the source PDF (`../inputs/Kronika-Stara-Ves-1923-1991-epdf.pdf`) into a single large markdown file (`data/kronika.md`). It also performs initial cleanup, such as fixing soft hyphens and page breaks.
-   `md_process.py`: Reads `data/kronika.md`, recognizes entities (names, years, addresses, events) based on regex rules and data files, and generates `data/index_data.jsonl` and the enriched `data/kronika_plus.md`.
-   `md_split.py`: Splits `data/kronika_plus.md` into individual chapter files based on the `data/chapter_mapping.jsonl` configuration. It also generates the index files for each entity type (e.g., names, years) and the `data/global_index.json`.
-   `md_generate_nav.py`: Automatically generates the navigation structure for the MkDocs website based on the processed chapter files and indexes.

### Helper Scripts
-   `help_build_vocab.py`: A script to help build a vocabulary from the text.
-   `help_diff_ent_bsforms.py`: A script to diff entity base forms.
-   `help_rebuild_base_form.py`: A script to rebuild the base form mappings.

## Workflows

### 1. Regenerate All Content
To run the entire data processing pipeline from scratch, execute the `full_cycle.bat` script. This will:
1.  Convert the PDF to markdown (`md_create.py`).
2.  Process the markdown and extract entities (`md_process.py`).
3.  Split the content into chapters and generate indexes (`md_split.py`).
4.  Generate the website navigation (`md_generate_nav.py`).
5.  Copy the generated files to the `docs-site/docs` directory.

### 2. Handling Source Content Changes
When the source `data/kronika.md` or any of the supporting data files (`chapter_mapping.jsonl`, `name_base_form.jsonl`, etc.) are modified, you should run the `run.bat` script. This script executes the main processing steps to update the website content:
1.  `md_process.py`
2.  `md_split.py`
3.  `md_generate_nav.py`
